{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the logistic regression, we transform the data $x$ by a linear transformation $x^T\\beta$ and determine the class label by a non-linear sigmoid function. When $x$ are not good features, however, we can construct new features $z$ from $x$ through some non-linear transformation. And the class label can be inferred based on the transformed features.\n",
    "\n",
    "* $y_i \\sim \\textrm{Ber}(p_i)$ \n",
    "* $p_i = \\textrm{sigmoid}(z_i^T\\beta)$\n",
    "\n",
    "Now the question becomes, how to find the transformed features $z_i$. This can be problem-dependent and usually requires domain knowledge in real-world applications.\n",
    "Here, for simplicity, we assume $z_i$ is constructed by a logistic regression on $x_i$.\n",
    "\n",
    "* $z_{ik} = \\textrm{sigmoid}(x_i^T \\alpha_k)$.\n",
    "\n",
    "Now, we obtained a very simple two-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the sigmoid function first for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Since the top-most layer is still a logistic regression, we can define the loss function as the negative log likelihood. Our objective is to minimize this loss.\n",
    "\n",
    "$$\\ell  = - \\frac{1}{n}\\sum_{i=1}^{n}( y_i \\log p_i + (1-y_i) \\log (1-p_i) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_neg_likelihood(p, y):\n",
    "    \"\"\"\n",
    "    Average negative log likelihood as loss function.\n",
    "\n",
    "    Args:\n",
    "        p: predicted probability of shape (n, 1).\n",
    "        y: true values of shape (n, 1).\n",
    "\n",
    "    Returns:\n",
    "        L: the loss. It is a scalar.\n",
    "        dp: the derivative of loss to predictions p of shape (n, 1)\n",
    "    \"\"\"\n",
    "    loss = -np.sum( y*np.log(p) + (1-y)*np.log(1-p) ) / p.shape[0]\n",
    "    dy = (1 - y)/(1 - p) - (y / p)\n",
    "    dy = dy / y.shape[0]\n",
    "    return loss, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also use the least squares error.\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} (y_i - p_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_l2(p, y):\n",
    "    \"\"\"\n",
    "    Use L2-norm (least squares error) as loss function.\n",
    "\n",
    "    Args:\n",
    "        p: predicted probability of shape (n, 1).\n",
    "        y: true values of shape (n, 1).\n",
    "\n",
    "    Returns:\n",
    "        L: the loss. It is a scalar.\n",
    "        dp: the derivative of loss to predictions p of shape (n, 1)\n",
    "    \"\"\"\n",
    "    loss = np.sum(np.square(y - p))\n",
    "    dp = -(y - p)\n",
    "    return loss, dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the implementation, we returned two values: loss and derivative.\n",
    "Essentially we will only use the derivative to calculate gradient when apply gradient descent to minimize the loss.\n",
    "But usually we want to keep tracking the loss to determine when to terminate the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "To get the final output from the network, we need to transform the data input data $x$ through the layers in the network. This is called _feedforward_. The parameters in each layer is called _weights_ and _bias_.\n",
    "\n",
    "In our two-layer network case, the operation in each layer is simply a linear transformation followed by a sigmoid function. In the implementation, we abstract this to become a `feed_forward(X, W, b)` function which can be used by both layers, where `X` is the input data to this layer, `W` and `b` are the weights and bias, respectively. The return value of this function is the layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Feed forward a logistic regression layer.\n",
    "\n",
    "    Args:\n",
    "        X: Input data of shape (n, p).\n",
    "            n = # of data examples.\n",
    "            p = # of predictors.\n",
    "        W: Weight matrix in this layer of shape (p, d)\n",
    "            p = # of predictors.\n",
    "            d = # of nodes in the next layer.\n",
    "        b: bias terms of shape (d, )\n",
    "\n",
    "    Returns:\n",
    "        y = sigmoid(XW + b.T)  of shape (n, d)\n",
    "    \"\"\"\n",
    "    p = sigmoid(X.dot(W) + b)\n",
    "    params = (X, W, b, p)\n",
    "    return p, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "In our network, the parameters we want to update are $\\beta$ and $\\alpha_k$'s.\n",
    "\n",
    "Suppose the top layer loss function is $\\ell$, we want to calculate the following two derivatives:\n",
    "\n",
    "* $\\dfrac{\\partial \\ell}{\\partial \\beta}$,\n",
    "* $\\dfrac{\\partial \\ell}{\\partial \\alpha} = \\dfrac{\\partial \\ell}{\\partial z} \\dfrac{\\partial z}{\\partial \\alpha}$.\n",
    "\n",
    "Note that when calculate the gradient of parameters in the lower layer,\n",
    "we are actually using the chain rule where the loss is propagated back from upper layers. \n",
    "Hence, this is called _backpropagation_.\n",
    "\n",
    "\n",
    "Then we use these gradient to update the parameters.\n",
    "\n",
    "* $\\beta_{t+1} = \\beta_t + \\dfrac{\\partial \\ell}{\\partial \\beta}$,\n",
    "* $\\alpha_{k,t+1} = \\alpha_{k,t} + \\dfrac{\\partial \\ell}{\\partial \\alpha}$.\n",
    "\n",
    "In the implementation, we want to abstract the operations.\n",
    "Suppose $X$ and $y$ are input and output of one layer, $W$ and $b$ are weights and bias in this layer, respectively, the backpropagation operation of the layer simply computes three values:\n",
    "\n",
    "* $\\dfrac{dy}{dW}$ and $\\dfrac{dy}{db}$ are the derivative to the parameters in this layer.\n",
    "* $\\dfrac{dy}{dX}$ is the derivative to the input of this layer, which will be propagated back to lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(dy, params):\n",
    "    \"\"\"\n",
    "    Back propagation of a logistic regression layer.\n",
    "\n",
    "    Args:\n",
    "        dy: Derivatives output of this layer of shape (n, d)\n",
    "        params: A tuple of parameters at the current layer.\n",
    "                In logistic regression layer, we have (X, W, b, p).\n",
    "\n",
    "    Returns:\n",
    "        dX: Derivatives to the input for BP to previous layers.\n",
    "            Same shape of X: (n, p)\n",
    "\n",
    "        dW: Derivatives to weights W at this layer.\n",
    "            Same shape of W: (p, d)\n",
    "\n",
    "        db: Derivatives to bias b at this layer.\n",
    "            Same shape of b: (d, )\n",
    "    \"\"\"\n",
    "    # Unpack params\n",
    "    X, W, b, p = params\n",
    "\n",
    "    dactivate = dsigmoid(p)\n",
    "\n",
    "    dW = X.T.dot(dy * dactivate)\n",
    "    db = np.sum(dy * dactivate, axis=0)\n",
    "    dX = (dy * dactivate).dot(W.T)\n",
    "\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the training phase, we apply gradient descent which iteratively update the parameters in the network.\n",
    "Conceptually, each iteration consists of three operations:\n",
    "\n",
    "* feed forward to get current network output\n",
    "* backpropagation to get gradients\n",
    "* update the paramters along the gradients\n",
    "\n",
    "Here we also implement a `accuracy()` function to calculate the classification accuracy of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(p, y):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions against truth labels.\n",
    "\n",
    "        Accuracy = # of correct predictions / # of data.\n",
    "\n",
    "    Args:\n",
    "        p: predictions of shape (n, 1)\n",
    "        y: true labels of shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "        accuracy: The ratio of correct predictions to the size of data.\n",
    "    \"\"\"\n",
    "    return np.mean((p > 0.5) == (y == 1))\n",
    "\n",
    "def train(X, y, num_hidden=100, num_classes=1, num_iterations=2000,\n",
    "            loss_function=loss_l2,\n",
    "            learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Make prediction on X using model by doing forward feeding.\n",
    "\n",
    "    Args:\n",
    "        X: The data matrix of shape (n, p)\n",
    "        y: The label for data of shape (n, p)\n",
    "        num_hidden: number of neurons in the hidden layer.\n",
    "        num_classes: 1 as we are doing binary classification.\n",
    "        num_iterations: number of iterations to do training.\n",
    "        loss_function: what loss function to use.\n",
    "        learning_rate: learning rate for updating parameters.\n",
    "\n",
    "    Returns:\n",
    "        model: tuple of trained parameters (W1, b1, W2, b2).\n",
    "        all_loss: a list of losses after each iteration.\n",
    "        accuracies: a list of training accuracies after each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Initialize parameters.\n",
    "    # Weights and bias two layers, random numbers centered at 0.\n",
    "    W1 = 2*np.random.randn(p, num_hidden) - 1\n",
    "    b1 = 2*np.random.randn(num_hidden, ) - 1\n",
    "    W2 = 2*np.random.randn(num_hidden, num_classes) - 1\n",
    "    b2 = 2*np.random.randn(num_classes, ) - 1\n",
    "\n",
    "    all_loss = []\n",
    "    accuracies = []\n",
    "\n",
    "    for it in xrange(num_iterations):\n",
    "        # Feed forward.\n",
    "        Z, params1 = feed_forward(X, W1, b1)\n",
    "        y_out, params2 = feed_forward(Z, W2, b2)\n",
    "\n",
    "        # Calculate the loss.\n",
    "        loss, dy = loss_function(y_out, y)\n",
    "\n",
    "        # Back propagation.\n",
    "        dZ, dW2, db2 = back_prop(dy, params2)\n",
    "        dX, dW1, db1 = back_prop(dZ, params1)\n",
    "\n",
    "        # Update parameters along the gradients.\n",
    "        W1 -= (learning_rate * dW1)\n",
    "        b1 -= (learning_rate * db1)\n",
    "        W2 -= (learning_rate * dW2)\n",
    "        b2 -= (learning_rate * db2)\n",
    "\n",
    "        # Save loss and accuracy for plotting the curves.\n",
    "        all_loss.append(loss)\n",
    "        accuracies.append(accuracy(y_out, y))\n",
    "\n",
    "        # Print progress every 100 iterations.\n",
    "        if it % 100 == 0:\n",
    "            print 'After iteration %d, loss = %f, accuracy = %f' % (\n",
    "                it, loss, accuracies[-1])\n",
    "\n",
    "    # Print the training accuracy at the very last.\n",
    "    print 'Accuracy on training data: %f' % accuracies[-1]\n",
    "\n",
    "    # Pack the trained parameters and return it.\n",
    "    model = (W1, b1, W2, b2)\n",
    "    return model, all_loss, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Testing is simply to feed forward the entire network using the testing data \n",
    "and retrieve the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(X, model):\n",
    "    \"\"\"\n",
    "    Make prediction on X using model by doing forward feeding.\n",
    "\n",
    "    Args:\n",
    "        X: The data matrix of shape (n, p)\n",
    "        model: tuple of trained parameters (W1, b1, W2, b2).\n",
    "\n",
    "    Returns:\n",
    "        y_predict: The prediction of shape (n, 1).\n",
    "\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = model\n",
    "    Z, _ = feed_forward(X, W1, b1)\n",
    "    y_predict, _ = feed_forward(Z, W2, b2)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example with hand-written digits\n",
    "\n",
    "Here we use the digits dataset from scikit-learn to train and test our two-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAD8CAYAAABErA6HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIZgBIBiCGQCCIZgBIBiCGQCC+X9QUmJIyuklRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1071b7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "def load_digits(subset=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Load digits and labels from digits.csv.\n",
    "\n",
    "    Args:\n",
    "        subset: A subset of digit from 0 to 9 to return.\n",
    "                If not specified, all digits will be returned.\n",
    "        normalize: Whether to normalize data values to between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        digits: Digits data matrix of the subset specified.\n",
    "                The shape is (n, p), where\n",
    "                    n is the number of examples,\n",
    "                    p is the dimension of features.\n",
    "        labels: Labels of the digits in an (n, ) array.\n",
    "                Each of label[i] is the label for data[i, :]\n",
    "    \"\"\"\n",
    "    # load digits.csv, adopted from sklearn.\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('digits.csv')\n",
    "\n",
    "    # only keep the numbers we want.\n",
    "    if subset is not None:\n",
    "        df = df[df.iloc[:,-1].isin(subset)]\n",
    "\n",
    "    # convert to numpy arrays.\n",
    "    digits = df.iloc[:,:-1].values.astype('float')\n",
    "    labels = df.iloc[:,-1].values.astype('int')\n",
    "\n",
    "    # Normalize digit values to 0 and 1.\n",
    "    if normalize:\n",
    "        digits -= digits.min()\n",
    "        digits /= digits.max()\n",
    "\n",
    "    # Change the labels to 0 and 1.\n",
    "    for i in xrange(len(subset)):\n",
    "        labels[labels == subset[i]] = i\n",
    "\n",
    "    labels = labels.reshape((labels.shape[0], 1))\n",
    "    return digits, labels\n",
    "\n",
    "\n",
    "def display_samples(digits, labels, n_samples=5):\n",
    "    \"\"\"\n",
    "    Display random samples from the training set for each label.\n",
    "    \"\"\"\n",
    "    distinct_label = np.unique(labels[:,0])\n",
    "\n",
    "    fig_rows = len(distinct_label)\n",
    "    fig_cols = n_samples\n",
    "    fig_num = 1\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    fig.suptitle('Random samples of training data')\n",
    "    for label in distinct_label:\n",
    "        # random choose samples to display\n",
    "        choice = np.random.choice(np.ix_(labels[:,0] == label)[0], n_samples)\n",
    "        for idx in choice:\n",
    "            ax = fig.add_subplot(fig_rows, fig_cols, fig_num)\n",
    "            fig.subplots_adjust(wspace=0, hspace=0)\n",
    "            ax.set_title(labels[idx])\n",
    "            ax.imshow(digits[idx].reshape(8,8), cmap=plt.cm.gray_r, interpolation='none')\n",
    "            ax.axis('off')\n",
    "            fig_num += 1\n",
    "    plt.show()\n",
    "\n",
    "# Load digits and labels.\n",
    "digits, labels = load_digits(subset=[3, 5], normalize=True)\n",
    "\n",
    "# Uncomment the following line to visualize samples from data.\n",
    "display_samples(digits, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we usually split the dataset into a training set and a testing test\n",
    "and only train the model using the training set.\n",
    "The model's performance on testing set gives us a general idea on how good our model can do in new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training 255\n",
      "# testing 110\n",
      "After iteration 0, loss = 114.028019, accuracy = 0.494118\n",
      "After iteration 100, loss = 20.677604, accuracy = 0.894118\n",
      "After iteration 200, loss = 6.524035, accuracy = 0.984314\n",
      "After iteration 300, loss = 3.366022, accuracy = 0.996078\n",
      "After iteration 400, loss = 2.295989, accuracy = 0.996078\n",
      "After iteration 500, loss = 1.754197, accuracy = 0.996078\n",
      "After iteration 600, loss = 1.432228, accuracy = 0.996078\n",
      "After iteration 700, loss = 1.218119, accuracy = 0.996078\n",
      "After iteration 800, loss = 1.062658, accuracy = 0.996078\n",
      "After iteration 900, loss = 0.942025, accuracy = 0.996078\n",
      "After iteration 1000, loss = 0.843852, accuracy = 0.996078\n",
      "After iteration 1100, loss = 0.761368, accuracy = 0.996078\n",
      "After iteration 1200, loss = 0.690674, accuracy = 0.996078\n",
      "After iteration 1300, loss = 0.629375, accuracy = 0.996078\n",
      "After iteration 1400, loss = 0.575861, accuracy = 0.996078\n",
      "After iteration 1500, loss = 0.528940, accuracy = 1.000000\n",
      "After iteration 1600, loss = 0.487655, accuracy = 1.000000\n",
      "After iteration 1700, loss = 0.451204, accuracy = 1.000000\n",
      "After iteration 1800, loss = 0.418898, accuracy = 1.000000\n",
      "After iteration 1900, loss = 0.390147, accuracy = 1.000000\n",
      "Accuracy on training data: 1.000000\n",
      "Accuracy on testing data: 0.981818\n"
     ]
    }
   ],
   "source": [
    "def split_samples(digits, labels):\n",
    "    \"\"\"Split the data into a training set (70%) and a testing set (30%).\"\"\"\n",
    "    num_samples = digits.shape[0]\n",
    "    num_training = int(round(num_samples * 0.7))\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    training_idx, testing_idx = indices[:num_training], indices[num_training:]\n",
    "    return (digits[training_idx], labels[training_idx],\n",
    "            digits[testing_idx], labels[testing_idx])\n",
    "\n",
    "\n",
    "training_digits, training_labels, testing_digits, testing_labels = split_samples(digits, labels)\n",
    "print '# training', training_digits.shape[0]\n",
    "print '# testing', testing_digits.shape[0]\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits, labels, test_size=0.3, random_state=1)\n",
    "\n",
    "# Train a net and display training accuracy.\n",
    "model, all_loss, all_accuracies = train(X_train, y_train, loss_function=loss_l2)\n",
    "\n",
    "# Evaluate on the testing set.\n",
    "y_predict = test(X_test, model)\n",
    "testing_accuracy = accuracy(y_predict, y_test)\n",
    "print 'Accuracy on testing data: %f' % testing_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following code segment plots the loss and training accuracy in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAFHCAYAAAAcFhBNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4ZHV97/v3t+eGpmlaTNM0SGNkVgYTEHBgRzzacKIY\nj4h4jhrjGINDcjWCNyfunJPro+Z6jkk0XhIFcUI5GseI0CJbNEEQGYSGBlppbYZukGbqefreP9ba\nUGxq967aQ61aVe/X89Sz11q11qrv2rv58anfbw2RmUiSJKl7Tau6AEmSJO2egU2SJKnLGdgkSZK6\nnIFNkiSpyxnYJEmSupyBTZIkqctNWWCLiAsiYl1E3Nyw7O8i4raIuCki/jUi9m5477yIuDMiVkbE\nS6eqLklqV7P2rMk6/1C2YTdFxHGdrE9S75vKHrYLgWUjll0OHJWZxwB3AOcBRMSRwFnAkeU2/xQR\n9v5J6hbN2rPHRcTpwLMy8xDgbcCnO1WYpP4wZaEoM38MPDRi2fLM3FXOXgMcUE6fAVycmdszczWw\nCjhhqmqTpHY0a89GeAVwUbnuNcCCiFjUidok9Ycqe7H+BPheOb0/cHfDe3cDSzpekSSNzxJgTcP8\n3TzxhVSSJqySwBYR/zewLTO/vJvVfGaWpDqJEfO2YZImzYxOf2BE/DFwOnBqw+J7gAMb5g8ol43c\n1gZQ6kOZOTIMdRvbMElNTVb71dHAFhHLgPcDp2Tmloa3vg18OSL+F8XQwiHAtc320c8Pqx8cHGRw\ncLDqMirRz8cO/X38Ed2e1YCiDTsH+EpEnAg8nJnrmq3YyTbsM5+Bv/3bjn3cbj388CALFgxWXUZl\n+vX416+HF71okO9+d7DqUioxme3XlAW2iLgYOAXYNyLWAB+iuCp0FrC8PIirM/OdmXlrRFwC3Ars\nAN6Z/ZzMJHWVUdqzmQCZeX5mfi8iTo+IVcBG4E3VVVvYsQOuuQb+5E/gDW+ouhr4xCfgve+tuorq\n9PPxX3BB1RX0hikLbJl5dpPFo/7ZMvPDwIenqh5JGq9R2rOR65zTiVpa9Za3wCWXwDe+AUuXVl0N\nLFjQHXVUpZ+Pf5o36ZoUHT+HTeM3MDBQdQmV6edjB49frdu1C667DlasgG9/G17ykqorKvT7v+F+\nPv5+PvbJFHUaeYwIR0qlPhMRdbjooCWdaMNuuAFe9CJ47nPhi1+EAw8cextJU2My2y972CSp5u66\nCy67rJi+/XZ43vPgBz+otiZJk8vAJkk1d8EFRWB77nOL+be8pdp6JE0+h0QldTWHRMd26KHwrncV\nL0ndYzLbLwObpK5mYNu9LVtg7ly4915YvHhSdy1pgiaz/fJiW0mqqf/+3+EP/gD239+wJvU6A5sk\n1dS3vgVvextccUXVlUiaag6JSupqDok2d999Rc/a2rWwaNGk7FLSJHNIVJL63Jo1cMwxhjWpX9Qu\nsNnBJknFQ7UNa1L/qF1g27at6gokqVpbtsBVV8HChVVXIqlTahfYNm2qugJJqtby5cXNck8/vepK\nJHVK7S46WLMmOeCAqiuR1CledPBkmfCf/zPsuy98/vOTVJikKdHXzxK1h01SP3vwQbj0UhgaqroS\nSZ1UuyHRjRurrkCSqrN+PRxyCJxyStWVSOokA5sk1cj69V5sIPWj2gU2h0Ql9bOf/xxmzqy6Ckmd\nVrvAZg+bpH62aRMcdVTVVUjqtNoFNnvYJPWzzZvh6U+vugpJnVa7wGYPm6R+9utfw9y5VVchqdMM\nbJJUIxdcAIceWnUVkjqtdoFty5aqK5CkamzfDjNmwH/5L1VXIqnTahfYfJaopH710EOwYAFETzz3\nQVI7ahfYtm6tugJJqsby5TBrVtVVSKqCgU2SauKHP4T/9J+qrkJSFWoX2BwSldSPVqwoLjh4+cur\nrkRSFWoX2Oxhk9SPrr0WnvMcOP30qiuRVAUDmyTVwFe/Cs9/vvdgk/pV7QKbQ6KS+tH69XDmmVVX\nIakqtQts9rBJ6kePPQb77Vd1FZKqYmCTpBpYvx4WLqy6CklVqV1gc0hUUr/JLALbPvtUXYmkqtQu\nsNnDJqnfbNgAs2cXL0n9ycAmSV3uoYccDpX6Xe0Cm0OikvrJtm3woQ85HCr1uxlVF9Aue9gk9ZPV\nq+F734Mvf7nqSiRVyR42SepSjz0Gf/zH8MxnwqmnVl2NpCpNWWCLiAsiYl1E3NywbGFELI+IOyLi\n8ohY0PDeeRFxZ0SsjIiXjrZfe9gkdVpELCvbpjsj4gNN3t8nIr4RETdFxDURcdRkfO5998FvfmPv\nmqSp7WG7EFg2Ytm5wPLMPBS4opwnIo4EzgKOLLf5p4hoWpuBTVInRcR04JMUbdORwNkRccSI1T4I\nXJ+ZxwBvAP5+Mj578+biYoODD56MvUmqsykLbJn5Y+ChEYtfAVxUTl8EvLKcPgO4ODO3Z+ZqYBVw\nQrP9OiQqqcNOAFZl5urM3A58haLNanQEcCVAZt4OLI2Ip0/0g7ds8dmhkgqdPodtUWauK6fXAYvK\n6f2BuxvWuxtY0mwH9rBJ6rAlwJqG+Wbt003AqwAi4gTgIOCAiX7wxo2wxx4T3YukXlDZVaKZmRGR\nu1ul2cKtWwcZHCymBwYGGBgYmPTaJFVnaGiIoaGhqstotLt2athHgL+PiBuAm4EbgJ3NVhwcbsAY\nuw276irYe+82KpVUqalsvyKzlbZonDuPWAp8JzOfU86vBAYyc21ELAauzMzDI+JcgMz8SLne94EP\nZeY1I/aXkOzaBRFTVrakLhIRZGZl/8VHxInAYGYuK+fPA3Zl5kd3s81dwHMyc8OI5dlOmztrFnzk\nI/AXfzG+2iVVazLbr04PiX4beGM5/Ubgmw3LXxsRsyLiYOAQ4NpmO5g2DXY2/d4qSVPiOuCQiFga\nEbMoLpD6duMKEbF3+R4R8VbgRyPDWrtWrIDt2+FP/3Qie5HUK6ZsSDQiLgZOAfaNiDXAX1MMG1wS\nEW8GVgOvAcjMWyPiEuBWYAfwztG+hs6cCTt2wIza3fJXUh1l5o6IOAe4DJgOfDYzb4uIt5fvn09x\n9ejnytM8bgHePNHPveACOPFELzqQVJjSIdHJFhG5557J2rUwb17V1UjqhKqHRCdTO0Oiz342vPe9\n8Ja3THFRkqZMnYdEJ2zGjGKYQJJ62YoVPt1A0hNqGdh27Ki6CkmaOrfcUvw86KBq65DUPWoX2IbP\nYZOkXnXrrfDiFxcXWUkS1DCw2cMmqdf95CfwO79TdRWSukktA5vnsEnqVdu2wdq1cMopVVciqZvU\nLrA5JCqpl33mM3DllXD00VVXIqmb1C6wOSQqqZfdcgu8851w8slVVyKpm9QysDkkKqlX3XUX7Ltv\n1VVI6ja1C2wOiUrqVRs3FsOhxx5bdSWSuk3tAptDopJ61cqVsHUrHHdc1ZVI6jYGNknqElu2wEkn\n+eg9SU9Vy8DmOWySetHmzT7sXVJztQtsnsMmqVdt2mRgk9Rc7QKbQ6KSetX69bBgQdVVSOpGtQxs\nDolK6kU/+QnMmVN1FZK6Ue0Cm0OiknrVqlXwwhdWXYWkblS7wOaQqKRe9OCD8KMfeQ82Sc3VMrA5\nJCqp1/z2t3DIIXDMMVVXIqkb1TKw2cMmqdc88gjsvXfVVUjqVrULbJ7DJqkXbdgAe+1VdRWSulXt\nAps9bJJ60caNPuFA0uhqGdg8h01Sr9mwwcAmaXS1C2wOiUrqRQY2SbtTu8DmkKikXvTTn1ZdgaRu\nVsvA5pCopF7zwANw8slVVyGpW9UysNnDJqmXrF8P3/kOHHlk1ZVI6lYGNkmq2P33w6GHwgknVF2J\npG5Vy8C2c2fVVUjS5Fm/HhYurLoKSd2sloHNHjZJveTKK4urRCVpNDOqLqBdBjZJveav/qrqCiR1\nO3vYJEmSupw9bJJUsRe8AF71qqqrkNTN7GGTpIrNn19cJSpJozGwSVLFNm+GuXOrrkJSNzOwSVLF\nDGySxmJgk6SKbd4Mc+ZUXYWkbmZgk6SKbdwIe+5ZdRWSulklgS0izouIFRFxc0R8OSJmR8TCiFge\nEXdExOURsaDZtgY2SZ0WEcsiYmVE3BkRH2jy/r4R8f2IuDEibomIP25135mwahXss8+kliypx3Q8\nsEXEUuCtwHMz8znAdOC1wLnA8sw8FLiinH8KA5ukToqI6cAngWXAkcDZEXHEiNXOAW7IzGOBAeDj\nEdHSbZPWri1++mgqSbtTRQ/bo8B2YI+yQdsDuBd4BXBRuc5FwCubbWxgk9RhJwCrMnN1Zm4HvgKc\nMWKd+4D55fR84MHMbKml2roVDjoIpk+ftHol9aCOB7bMXA98HPgNRVB7ODOXA4syc1252jpgUbPt\nDWySOmwJsKZh/u5yWaN/AY6KiHuBm4D3tLrzbdtg1qwJ1yipx1UxJPq7wHuBpcD+wLyI+G+N62Rm\nAtlsewObpA5r2haN8EHgxszcHzgW+FRE7NXKzg1sklpRxaOpfh/4j8x8ECAi/hU4CVgbEftl5tqI\nWAzc32zjL35xkDvvhMFBGBgYYGBgoENlS+qEoaEhhoaGqi6j0T3AgQ3zB1L0sjU6Gfh/ADLzlxFx\nF3AYcN3InQ0ODj4+PTAwwPz5AwY2qUdMZfsVRWdW50TEMcCXgOOBLcDngGuBgyjO+/hoRJwLLMjM\nc0dsm1ddlXzwg/DjH3e0bEkViQgyMyr8/BnA7cCpFKdxXAucnZm3Nazzv4BHMvNvImIR8HPg6PIU\nkMZ95cg29+qr4c//HH760yk+EEkdN5ntV8d72DLzpoj4PMU3z13A9cA/A3sBl0TEm4HVwGuabe+Q\nqKROyswdEXEOcBnFVe2fzczbIuLt5fvnAx8GLoyImyhONfnLkWFtNA8/DAua3sRIkp5QxZAomfkx\n4GMjFq8HXjLWtgY2SZ2WmZcCl45Ydn7D9G+Bl49n3+vXew82SWPzSQeSVKHVqw1sksZmYJOkCt1x\nB8ybV3UVkrqdgU2SKrRjBxx9dNVVSOp2BjZJqtDmzTB3btVVSOp2BjZJqtDKlTBnTtVVSOp2BjZJ\nqtADD8ABB1RdhaRuZ2CTpArNng1Pe1rVVUjqdgY2SaqQzxKV1AoDmyRVyMAmqRUGNkmqSCY88oiB\nTdLYDGySVJGf/az4OXt2tXVI6n4GNkmqyKOPwotfDNOnV12JpG5Xu8A2fXoR2DKrrkSSJmbrVnvX\nJLWmdoFt2rTitWtX1ZVI0sQY2CS1qnaBDRwWldQbvEJUUqsMbJJUEXvYJLXKwCZJFTGwSWqVgU2S\nKuKQqKRWGdgkqSL2sElqlYFNkiqybZuBTVJrDGySVJGtWx0SldQaA5ukvhARr4iIrmrzHBKV1Kqu\narxaZWCTNA5nAasi4mMRcXjVxYBDopJaZ2CT1Bcy878CxwG/Aj4XEVdHxNsiYq+qanJIVFKrDGyS\n+kZmPgJ8DfgqsD/wR8ANEfHuKuq580572CS1xsAmqS9ExBkR8Q1gCJgJHJ+ZpwFHA39RRU3r18O+\n+1bxyZLqZkbVBYyHgU3SOLwK+N+ZeVXjwszcFBFvqaKg7dvhmc+s4pMl1Y2BTVK/+BvgvuGZiJgL\nLMrM1Zn5gyoK2rAB5s2r4pMl1Y1DopL6xSXAzob5XRTns1XGwCapVQY2Sf1iRmZuG57JzK0U57JV\nxsAmqVUGNkn94rcRccbwTDn926qK2bkTNm+GPfaoqgJJdeI5bJL6xTuAL0XEJ8v5u4HXV1XMI4/A\n/PkwrZZfmyV1moFNUl/IzFXA88ob5WZmbqiynkcfLQKbJLVizMAWEfOAzZm5MyIOAw4DLs3M7VNe\n3SgMbJLGIyL+EDgSmBMRAGTm/6iilm3bfMqBpNa10hl/FTA7IpYAl1EMIXxuKosai4FNUrsi4nzg\nNcC7gSinD6qqnu3bYWallzxIqpNWAltk5iaKm07+U2aeCTx7asvavRkzisZOktpwcma+AVifmX8D\nnEgxYlAJA5ukdrR0umtEnAT8V+Df2tluqsyZA1u2VFmBpBraXP7cVI4Y7AD2q6qYbdsMbJJa18pF\nB+8FzgO+kZkrIuJ3gSuntqzdmzu3uBxektrwnYjYB/g74Oflsn+pqpjt2z2HTVLrxgxsmfkj4EcA\nETENeCAz3z2RD42IBcBngKOABN4E3Al8leKcktXAazLz4WbbG9gktaNsu36YmQ8BX4+IfwPmjNbG\nNNl+GfAJYDrwmcz86Ij330cxCgFFu3oEsO/u9u+QqKR2jDm0GREXR8T8iNgTuAW4LSL+coKf+/fA\n9zLzCOBoYCVwLrA8Mw8FrijnmzKwSWpHZu4CPtUwv6WNsDYd+CSwjOIK07Mj4ogR+/9/M/O4zDyO\nYkRiaKz9G9gktaOVc9GOzMxHgVcClwJLmcDNJiNib+CFmXkBQGbuyMxHgFcAF5WrXVR+XlMGNknj\n8IOIeHUM38+jdScAq8qHxG8HvgKcsZv1XwdcPNZODWyS2tFKYJsRETMpAtR3ygYrJ/CZBwMPRMSF\nEXF9RPxL2Xu3KDPXleusAxaNtgMDm6RxeAfFA+C3RcRj5evRFrZbAqxpmL+7XPYUEbEH8DLg62Pt\n1MAmqR2tXHRwPsU5Zb8AroqIpcAjE/zM5wLnZObPIuITjBj+zMyMiKahcHBwkOuug/vug6GhAQYG\nBiZQiqRuMzQ0xNDQ0KTvNzPH+5j1dr6gvhz4ye6GQwcHBwH4+c9hzZoBYGCcZUnqNlPVfkFxj7X2\nNiiGE2aM90kHEbEfcHVmHlzOv4DinI9nAn+QmWsjYjFwZWYePmLbzEw+/3lYvhy+8IXxVCCpTiKC\nzGx3GLPZfl7UbHlmXjXGdicCg5m5rJw/D9g18sKD8r1vAF/NzK+Msq8cbnOHB2bbbIIl1chktV/Q\n2qOpFgAfAoYbuyHgfzDOXrYykK2JiEMz8w7gJcCK8vVG4KPlz2+Otg+HRCWNw1/yRG/ZHIpz034O\nvHiM7a4DDilHF+4FzgLOHrlSeX7uiyjOYZOkSdXKkOgFwM3AmRSPc3k9cCHFkw/G613AlyJiFvBL\nitt6TAcuiYg3U97WY7SNDWyS2pWZf9g4HxEHUlyxPtZ2OyLiHIpH800HPpuZt0XE28v3zy9XfSVw\nWWa21DqddRY89FA7RyCpn7US2H43MxvD2WBE3DSRD83Mm4Djm7z1kla233NP2LRpIhVIEndT3C9t\nTJl5KcVV8o3Lzh8xfxFPXOk+pmc8A447rtW1JfW7VgLb5oh4YWb+GB4/56zSuLRgATzc0h2UJKkQ\nEf/YMDsNOJYnnnjQcbt2wfTpVX26pLppJbC9A/h8eX4GwEMU55hVZp99YP36KiuQVEM/54lz2HYA\nX87Mf6+qmJ07YVqlT2WWVCetPJrqRuDo4cCWmY9ExHuBCQ2LTsTChZ77IaltXwM2Z+ZOKJ5gEBF7\nZGYlIwY7d9rDJql1LX+/y8xHyicSAPxfU1RPS/baqziHbfu4biwiqU/9AJjbML9HuawSDolKakct\nO+QjPI9NUtvmZOaG4ZnMfIwitFXCIVFJ7ahtc7FwoeexSWrLxoj4veGZiPh9oLIbBDkkKqkdo57D\nFhEbGP2RLJV9Kx3mhQeS2vReins93lfOL6a4CW4lDGyS2jFqYJvAc/c64nd+B+6/v+oqJNVF+ezi\nI4DDykW3Z+a2qurZtcshUUmtq21zsf/+cO+9VVchqS7KpxXsmZk3Z+bNwJ4R8c6q6rGHTVI7ahvY\nliwxsElqy1sz8/EbApXTb6uqGAObpHbUNrDtvz/cc0/VVUiqkWkR8XibFxHTgZlVFeNtPSS1o5Un\nHXQlh0Qlteky4CsRcT4QwNuB71dVjLf1kNSO2gY2h0QltekDFEOgf0pxBfwvKK4UrYRDopLaUdvv\ndw6JSmpH+Uiqa4DVwAnAqcBtVdXjkKikdtS2h+1pT4PHHoMtW2DOnKqrkdStIuIw4GyKe649APwf\nIDJzoMq6HBKV1I7aNhfTpsHixQ6LShrTbcBzgZdl5osy8x+BnRXX5JCopLbUNrCB57FJasmrKB5B\ndVVE/H8RcSrFRQeVckhUUjtqHdi8UlTSWDLzm5l5FvBs4MfAnwNPj4hPR8RLq6rLIVFJ7ah1c7Fk\niRceSGpNZm7IzC9l5h8CBwI3AOdWVY9DopLaUevAZg+bpPHIzPWZ+c+Z+eKqanBIVFI7ah3Y7GGT\nVFcOiUpqR62bC3vYJNWVQ6KS2lHrwGYPm6S6MrBJaketA9twD1tm1ZVIUnt27XJIVFLrat1c7LVX\n8Q31kUeqrkSS2mMPm6R21DqwgTfPlVRPBjZJ7ah9YPMh8JLqyNt6SGpH7QObPWyS6sjbekhqR+2b\nC3vYJNWRQ6KS2lH7wGYPm6Q6ckhUUjtqH9jsYZNURw6JSmpH7ZsLe9gk1ZFDopLaUfvAZg+bpDoy\nsElqR+0D2377wf33F+eDSFJdbN0Ks2dXXYWkuqh9YJs1C/beG37726orkaTW3HknPPAAzJ1bdSWS\n6qL2gQ1g8WK4776qq5Ck1tx/f/HTwCapVT0R2Pbbz8AmqT6Gg9qMGdXWIak+KgtsETE9Im6IiO+U\n8wsjYnlE3BERl0fEglb3ZQ+bpKkUEcsiYmVE3BkRHxhlnYGyTbslIoZ2t7+dO+H3fx8ipqRcST2o\nyh629wC3AlnOnwssz8xDgSvK+ZYY2CRNlYiYDnwSWAYcCZwdEUeMWGcB8Cng5Zn5bODVu9vntm3F\n+beS1KpKAltEHACcDnwGGP6O+QrgonL6IuCVre7PwCZpCp0ArMrM1Zm5HfgKcMaIdV4HfD0z7wbI\nzN1eBmVgk9SuqnrY/jfwfqDxZhyLMnNdOb0OWNTqzgxskqbQEmBNw/zd5bJGhwALI+LKiLguIl6/\nux0a2CS1q+OnvEbEHwL3Z+YNETHQbJ3MzIjIZu81Y2CTNIVaaYtmAs8FTgX2AK6OiJ9m5p3NVjaw\nSWpXFdconQy8IiJOB+YA8yPiC8C6iNgvM9dGxGLg/mYbDw4OPj49MDDAwMCAgU3qIUNDQwwNDVVd\nRqN7gAMb5g+k6GVrtAb4bWZuBjZHxFXAMcBTAtvg4CC33gqrVsHQUNGGSeoNU9l+RWbLHVmT/+ER\npwDvy8yXR8THgAcz86MRcS6wIDPPHbF+Nqt340bYd1/YtMmrrqReExFkZmX/ZUfEDOB2it6ze4Fr\ngbMz87aGdQ6nuDDhZcBs4BrgrMy8dcS+MjNZvBjWroUKm19JHTCZ7Vc33AVouMn6CHBJRLwZWA28\nptUd7LknzJwJjzwCC1q+GYgkjS0zd0TEOcBlwHTgs5l5W0S8vXz//MxcGRHfB35BcW7uv4wMa43W\nru1E5ZJ6SaU9bO0arYcN4LDD4JvfhCOOaPq2pJqquodtMg23YcMjATVqfiWNw2S2Xz3xpAPwwgNJ\n9XHaafBv/1Z1FZLqxMAmSR3mVaKS2mVgk6QOM7BJapeBTZI6bONG2GOPqquQVCc9E9j228/AJqn7\nZcKNN8LChVVXIqlOeiaw2cMmqQ42b4Zdu2Dp0qorkVQnBjZJ6qBt22DvvWFaz7S+kjqhZ5oMA5uk\nOvCCA0nj0TOBbZ99YOvW4vFUktStDGySxqNnAluEFx5I6n7bthWP0pOkdvRMYAOHRSV1P3vYJI2H\ngU2SOsjAJmk8DGyS1EHbtxvYJLXPwCZJHWQPm6TxMLBJUgcZ2CSNR08FtiVL4J57qq5CkkZnYJM0\nHj0V2JYuhbvuqroKSRqdgU3SePRcYFuzBnburLoSSWrOwCZpPHoqsM2ZA097msOikrqXgU3SePRU\nYAN45jMdFpXUvV77WrjkkqqrkFQ3PRfYDj7YwCZJknpLTwa2X/2q6iokqbl586quQFIdzai6gMl2\n8MHwwx9WXYUkNffGN8Lhh1ddhaS66bkeNs9hk9TNduyAGT33VVnSVOu5wOY5bJK6mYFN0nj0XGBb\nsgTWr4eNG6uuRJKeysAmaTx6LrBNnw7PehbccUfVlUjSUxnYJI1HzwU2KE7oXbmy6iok6akMbJLG\nw8AmSR1kYJM0HgY2SeogA5uk8ejZwHb77VVXIUlPZWCTNB49GdgOPbS46GDXrqorkaQnM7BJGo+e\nDGx77QULF8JvflN1JZL0ZAY2SePRk4EN4IgjYMWKqquQpCczsEkaj54NbMceCzfdVHUVkvRkBjZJ\n49HTge3GG6uuQpKebMeO4gbfktQOA5skddCGDTBvXtVVSKqbng1shx0Gd99dNI6S1C3Wry8uipKk\ndnQ8sEXEgRFxZUSsiIhbIuLd5fKFEbE8Iu6IiMsjYsFEPmfGDDjySLj55smpW1L/iohlEbEyIu6M\niA80eX8gIh6JiBvK11+Nti8Dm6TxqKKHbTvw55l5FHAi8GcRcQRwLrA8Mw8FrijnJ8RhUUkTFRHT\ngU8Cy4AjgbPLNmukH2XmceXrb0ffH8ydO0XFSupZHQ9smbk2M28spzcAtwFLgFcAF5WrXQS8cqKf\ndeyxcMMNE92LpD53ArAqM1dn5nbgK8AZTdaLVnZm75qk8aj0HLaIWAocB1wDLMrMdeVb64BFE93/\n8cfDNddMdC+S+twSYE3D/N3lskYJnBwRN0XE9yLiyNF2ts8+U1ChpJ5X2d2AImIe8HXgPZn5WMQT\nX04zMyMim203ODj4+PTAwAADAwOjfsZxx8GqVfDYY8XTDyR1v6GhIYaGhqouo1HTtmiE64EDM3NT\nRJwGfBM4tNmKDz88yHAzNlYbJqleprL9isxW2qJJ/tCImcB3gUsz8xPlspXAQGaujYjFwJWZefiI\n7bLdep//fPif/xNe/OJJKl5SR0UEmdnScOMUff6JwGBmLivnzwN2ZeZHd7PNXcDvZeb6EcvzDW9I\nLrpolA0l9ZTJbL+quEo0gM8Ctw6HtdK3gTeW02+k+IY6YSedBFdfPRl7ktSnrgMOiYilETELOIui\nvXpcRCwq2zYi4gSKL8Prn7orOOusqS5XUi+qYkj0+cB/A34REcOXBJwHfAS4JCLeDKwGXjMZH3bS\nSXDhhZOxJ0n9KDN3RMQ5wGXAdOCzmXlbRLy9fP984NXAn0bEDmAT8NrR9vfwwx0oWlLPqWRIdLzG\nMyR6770R9+MrAAAMX0lEQVRw9NHwwAPF5fSS6qXqIdHJFBH55S8nZ59ddSWSOqHWQ6Kdtv/+MH8+\nrFhRdSWSBGeeWXUFkuqo5wMbwKmnwhVXVF2FJBVPYZGkdvVFYHvJSwxskiSpvnr+HDYozl971rPg\nwQf9divVTa+dw1anNlfSxHgOW5ue/nRYuhR+9rOqK5EkSWpfXwQ2KM5j+8EPqq5CkiSpfX0T2E4/\nHb773aqrkCRJal9fnMMGsH07LFoEt9xS3OpDUj14DpukuvIctnGYORNOOw2+852qK5EkSWpP3wQ2\ngDPOgG99q+oqJEmS2tM3Q6IAjz4KBxwAa9bA3ntPYmGSpoxDopLqyiHRcZo/v7ha9Otfr7oSSZKk\n1vVVYAN4/evhC1+ougpJkqTW9dWQKMDWrbBkCVx/PTzjGZNUmKQp45CopLpySHQCZs+GV78avvjF\nqiuRJElqTd/1sEHxiKozz4Rf/hKmT5+EwiRNGXvYJNWVPWwTdPzxsHix92STJEn10JeBDeBd74J/\n/Meqq5AkSRpb3wa2V78aVq4sLj6QJEnqZn0b2GbNgnPPhb/+66orkSRJ2r2+vOhg2NatcMghcMkl\ncOKJk7ZbSZPIiw4k1ZUXHUyS2bOLHrb3vx927aq6GkmSpOb6OrABvOlNRU/b5z5XdSWSJEnN9fWQ\n6LAbb4SXvQxuugn222/Sdy9pAhwSlVRXDolOsmOPhXe8A173Oti5s+pqJEmSnszAVhq+WtSrRiVJ\nUrcxsJWmT4eLL4avfhU+/emqq5EkSXrCjKoL6CaLFsHll8MLX1gEuLe9reqKJEmSDGxP8cxnwo9+\nVFyEsGYNDA76gHhJklQtrxIdxbp1cOaZMHMmfOELsP/+HflYSSN4laikuvIq0Q5YtAh++EM45RQ4\n5hj4+Mdh27aqq5IkSf3IwLYbM2YUV43+5CdwxRVw+OHwqU/Bpk1VVyZJkvqJga0Fhx0G3/sefPGL\nsHw5HHQQvOtd8LOfgaMbkiRpqnkO2zjcdVcR3j7/+eIZpKefDqedBgMDsMceVVcn9RbPYZNUV5PZ\nfhnYJiATbr4ZLr206IG77jp4znPgpJPg5JPh+OPhGc+AafZjSuNmYJNUVwa2LrVxYxHarr4a/uM/\n4Prr4dFH4aij4NnPLl6HHAIHHwxLl8LcuVVXLHU/A5ukujKw1cj69bBiBdxyS/H65S/hV7+C3/wG\nnva0IrwddBAsXtz8NX8+RE/8r0oaHwObpLrq2cAWEcuATwDTgc9k5kdHvN8zjd3OnXDvvcX5cL/+\nNdx3X/PX9u2wcGHz1z77FD/nz4e99oJ584qfjdPz5nnjX9VbNwS2sdqmhvWOB64GXpOZ/9rk/Z5p\nwySNrScDW0RMB24HXgLcA/wMODszb2tYp+8au82b4aGHip66K64Y4qCDBli/vph/8MHi52OPPfHa\nsOHJ0xs3wuzZT4S3PfeEOXOK4di5c9ubnjMHZs0qXjNnPjHd+Gq2fMaMifcSDg0NMTAwMCm/0zrq\n5+OvOrC10jY1rLcc2ARcmJlfb7KvvmvDhvXzv2Ho7+Pv52OfzParmx5NdQKwKjNXA0TEV4AzgNt2\nt1GvGw5N++8PX/vaEO95z0Bb2+/aVYS+4RC3aRNs2VIs27x59OnNm4sw2Lh8y5aix2/btideI+eb\nLd+xY/QwN3Nm8Zoxo/lr+vTi5513DvGc5wyMut5Yr+H9NFs+fXpxYUjjz2bLRvs5WetGjB5s+7nB\n6wKttk3vAr4GHN/R6mqi3/8N9/Px9/OxT6ZuCmxLgDUN83cDz6uolp4xbVrRq7bnnrDfftXUsGvX\nEwGuWcDbsWPs15e+BK9+dWvrDr82bXry/M6dzdfbtat4b/hn4/RYPydz3czi79Us3G3dWty0OeKJ\ndYanW/3ZqW0m+/O6wJhtU0QsoQhxL6YIbP3ZjSZpynRTYLOB61HTphXDsrNnj38f119fPNu1l2WO\nHu4+/GF43/uKdYbXGw557fzs1DYT/bzh+S4ZPWylik8A52ZmRkQAPXGRhKTu0U3nsJ0IDGbmsnL+\nPGBX48m9EdEdxUrqqIrPYWulbfoVT4S0fSnOY3trZn57xL5sw6Q+04sXHcygOLH3VOBe4FqanNgr\nSZ3UbtsUERcC32l2lagkjVfXDIlm5o6IOAe4jOLS+c8a1iRVbbS2KSLeXr5/fqUFSuoLXdPDJkmS\npOa64xqsFkTEsohYGRF3RsQHqq5nKkTE6oj4RUTcEBHXlssWRsTyiLgjIi6PiAUN659X/j5WRsRL\nq6u8fRFxQUSsi4ibG5a1fawR8XsRcXP53t93+jjGa5TjH4yIu8u//w0RcVrDez1z/BFxYERcGREr\nIuKWiHh3ubxn//62X73VfkF/t2G2XxW1X5nZ9S+KYYhVwFJgJnAjcETVdU3Bcd4FLByx7GPAX5bT\nHwA+Uk4fWf4eZpa/l1XAtKqPoY1jfSFwHHDzOI91uHf4WuCEcvp7wLKqj20Cx/8h4C+arNtTxw/s\nBxxbTs+jOD/siF79+9t+9V77VR5D37Zhtl/VtF916WF7/MaVmbkdGL5xZS8aeTXJK4CLyumLgFeW\n02cAF2fm9ixu6LmK4vdUC5n5Y+ChEYvbOdbnRcRiYK/MvLZc7/MN23S1UY4fmt8OoqeOPzPXZuaN\n5fQGihvQLqF3//62X4Weab+gv9sw269q2q+6BLZmN65cUlEtUymBH0TEdRHx1nLZosxcV06vAxaV\n0/tT/B6G9cLvpN1jHbn8Hur/O3hXRNwUEZ9t6FLv2eOPiKUU39SvoXf//rZfhV5vv6B3/w23yvar\nMCV/+7oEtn65MuL5mXkccBrwZxHxwsY3s+g33d3vomd+Ty0cay/6NHAwcCxwH/DxasuZWhExD/g6\n8J7MfKzxvR77+/fKcYzF9qtBj/0bboXtV2mq/vZ1CWz3AAc2zB/Ik5NpT8jM+8qfDwDfoBgiWBcR\n+wGUXaj3l6uP/J0cUC6rs3aO9e5y+QEjltf2d5CZ92cJ+AxPDBH13PFHxEyKxu4LmfnNcnGv/v1t\nv+iL9gt699/wmGy/pv5vX5fAdh1wSEQsjYhZwFnAt8fYplYiYo+I2Kuc3hN4KXAzxXG+sVztjcDw\nP45vA6+NiFkRcTBwCMUJjHXW1rFm5lrg0Yh4XkQE8PqGbWqn/I982B9R/P2hx46/rPWzwK2Z+YmG\nt3r172/7Vej19gt699/wmGy/OvC3r+pKi3ZfFN3st1OcsHde1fVMwfEdTHElyY3ALcPHCCwEfgDc\nAVwOLGjY5oPl72Ml8LKqj6HN472Y4q7x2yjO73nTeI4V+D2KhmEV8A9VH9cEjv9PKE46/QVwU/kf\n7qJePH7gBcCu8t/6DeVrWS///W2/eqv9Kuvv2zbM9qua9ssb50qSJHW5ugyJSpIk9S0DmyRJUpcz\nsEmSJHU5A5skSVKXM7BJkiR1OQObJElSlzOwaUpExIby50ERcfYk7/uDI+b/fTL3L6m/2X6pGxnY\nNFWGb/B3MPC6djaMiBljrHLekz4o8/nt7F+SxmD7pa5jYNNU+wjwwoi4ISLeExHTIuLvIuLaiLgp\nIt4GEBEDEfHjiPgWxZ3SiYhvRsR1EXFLRLy1XPYRYG65vy+Uy4a/DUe575sj4hcR8ZqGfQ9FxP+J\niNsi4osV/B4k1Y/tl7rGWN8EpIn6APC+zHw5QNnAPZyZJ0TEbOAnEXF5ue5xwFGZ+ety/k2Z+VBE\nzAWujYivZea5EfFnmXlcw2cMfxt+FXAMcDTwdOBnEXFV+d6xwJHAfcC/R8TzM9OhCEm7Y/ulrmEP\nm6ZajJh/KfCGiLgB+CnF89eeVb53bUNjB/CeiLgRuBo4kOKhubvzAuDLWbgf+BFwPEWDeG1m3pvF\ns9huBJZO4Jgk9QfbL3UNe9hUhXMyc3njgogYADaOmD8VODEzt0TElcCcMfabPLWBHf72urVh2U78\nty9pfGy/VAl72DTVHgP2api/DHjn8Im5EXFoROzRZLv5wENlY3c4cGLDe9tHObH3x8BZ5XkmTwde\nBFzLUxtBSWqF7Ze6hildU2X4m+FNwM5yaOBC4B8ouvOvj4gA7gf+qFw/G7b/PvCOiLgVuJ1iWGHY\nPwO/iIifZ+brh7fLzG9ExEnlZybw/sy8PyKOGLFvmsxL0jDbL3WdKIbEJUmS1K0cEpUkSepyBjZJ\nkqQuZ2CTJEnqcgY2SZKkLmdgkyRJ6nIGNkmSpC5nYJMkSepyBjZJkqQu9/8Da91eVI2tvUwAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10480f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the curves.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(range(len(all_loss)), all_loss)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(range(len(all_accuracies)), all_accuracies)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
